{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多層パーセプトロンPython＆Chainerによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多層パーセプトロンモデル\n",
    "### 多層パーセプトロン数式\n",
    "### 多層パーセプトロン重み更新\n",
    "### Pythonの実装\n",
    "### Chainerの説明\n",
    "### Chainerの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 多層パーセプトロンの数式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of model](http://cdn-ak.f.st-hatena.com/images/fotolife/a/aidiary/20140122/20140122215625.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_k(\\bf{x}, \\bf{w}) = f \\Biggl( \\sum_{j=1}^M w_{kj}^{(2)} h \\biggl( \\sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)} \\biggr) + w_{k0}^{(2)} \\Biggr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xo = 1とすると："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_k(\\bf{x}, \\bf{w}) = f \\Biggl( \\sum_{j=0}^M w_{kj}^{(2)} h \\biggl( \\sum_{i=0}^D w_{ji}^{(1)} x_i \\biggr) \\Biggr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 出力ユニット活性化関数はf()、隠れユニット活性化関数はh()で表しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全学習データの二乗誤差の和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of model2](http://hokuts.com/wp-content/uploads/2016/05/error_func.png)\n",
    "$$E_n = \\frac{1}{2} \\sum_{k=1}^K (y_k - t_k)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力層の重み更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eを最小化するため、各wをどう求めるのは考えると、直接に二回微分で行うと、wがなくなるので使えない？\n",
    "従って、一回微分で求める方法→最急降下法/確率的勾配降下法にした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of model3](http://hokuts.com/wp-content/uploads/2015/11/gradient_dec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ある重みwに対して、Eを最小化するため、wの微分値、つまり傾きが正の時はwが負の方向に動き、負の時はwが正の方向に動くようになった。また、傾きの絶対値が大きいときは大きく変化し、小さいときは小さく変化します。ただ、これだと更新量が大きすぎてうまく収束しないので、小さな正の定数を決めて、微分値に掛けることで更新量を調整する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of model4](http://hokuts.com/wp-content/uploads/2016/05/w_l_i_j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 出力層i番目のユニットのj番目の重み、\n",
    "$$w_{i,j}^{(L)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### wの更新式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{ij}^{(L)} = w_{ij}^{(L)} - \\eta\\frac{\\partial E_n}{\\partial w_{ij}^{(L)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E=f(y); y=f(u); u=f(w)、偏微分を求める時それぞれ偏微分するので以下の式に変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(L)}} = \\frac{\\partial E_n}{\\partial y_{i}^{(L)}}\\frac{\\partial y_{i}^{(L)}}{\\partial u_{i}^{(L)}}\\frac{\\partial u_{i}^{(L)}}{\\partial w_{ij}^{(L)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### さらに計算すると、以下の式になった(過程は省略)→http://hokuts.com/2016/05/29/bp1/ に参考してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{ij}^{(L)} = w_{ij}^{(L)} - \\eta((y_{i}^{(L)} - t_{i}){f'(u_{i}^{(L)})}{y_{j}^{(L-1)}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### これは、出力層の重み更新式で、似たような計算で、隠れ層の重み更新式は以下のように:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of model5](http://hokuts.com/wp-content/uploads/2016/05/w_l-1_i_j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 計算後の式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{ij}^{(L-1)} = w_{ij}^{(L-1)} - \\eta(\\sum_{k} \\frac{\\partial E_n}{\\partial y_{k}^{(L)}}\\frac{\\partial y_{k}^{(L)}}{\\partial u_{k}^{(L)}}w_{k}^{(L)}{f'(u_{i}^{(L-1)})}{y_{j}^{(L-2)}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### kは出力層のユニット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://hokuts.com/wp-content/uploads/2016/05/o_l-1_i_error1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第L層の重み更新を先に計算すると、その計算結果の一部を第(L-1)層の重み更新の際に利用できた。プログラム上では一時変数に保持しておけば使える。それを使って第(L-1)層の重み更新の計算をし、さらにその計算結果の一部を第(L-2)層の重み更新に使い…ということができると、第1層の重みまですべてを更新することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### コード部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 必要なライブラリを導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>#coding:utf-8\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 定数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# 訓練データ数\n",
    "N = 50\n",
    "# 学習率\n",
    "ETA = 0.1　　#<b>どうの程度で重みwを調整するパラメータ</b>\n",
    "# ループ回数\n",
    "NUM_LOOP = 500  #<b>wを徐々に更新するので、一定の回数で回さないとうまく収束できない</b>\n",
    "# 入力層のユニット数（バイアス含む）\n",
    "NUM_INPUT = 2\n",
    "# 隠れ層のユニット数（バイアス含む）\n",
    "NUM_HIDDEN = 4\n",
    "# 出力層のユニット数\n",
    "NUM_OUTPUT = 1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ユニットの出力関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "def output(x, w1, w2):\n",
    "    \n",
    "    # <b>配列に変換して先頭にバイアスの1を挿入</b>\n",
    "    x = np.insert(x, 0, 1)\n",
    "    z = np.zeros(NUM_HIDDEN)\n",
    "    y = np.zeros(NUM_OUTPUT)\n",
    "    # 順伝播で出力を計算\n",
    "    for j in range(NUM_HIDDEN):\n",
    "        a = np.zeros(NUM_HIDDEN)\n",
    "        a[j] = np.dot(w1[j, :], x)\n",
    "        z[j] = np.tanh(a[j])     #<b>中間層の活性化関数はtan()</b>\n",
    "    for k in range(NUM_OUTPUT):\n",
    "        y[k] = np.dot(w2[k, :], z) #<b>出力層の活性化関数はそのまま線形関数の形</b>\n",
    "    return y, z\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データ, 重み作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "xlist = np.linspace(-5, 5, N).reshape(N, 1)\n",
    "#tlist = xlist * xlist    # x^2\n",
    "tlist = np.sin(xlist)     # sin(x) #<b>今回はsin()にした</b>\n",
    "#tlist = np.abs(xlist)    # |x|\n",
    "\n",
    "# <b>重みをランダムに初期化</b>\n",
    "w1 = np.random.random((NUM_HIDDEN, NUM_INPUT))\n",
    "w2 = np.random.random((NUM_OUTPUT, NUM_HIDDEN))\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習ループ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "ims = [] #グラッフ作成用\n",
    "for loop in range(NUM_LOOP):\n",
    "    for n in range(N):\n",
    "        # 隠れ層と出力層の出力配列を確保\n",
    "        z = np.zeros(NUM_HIDDEN)\n",
    "        y = np.zeros(NUM_OUTPUT)\n",
    "\n",
    "        # 誤差（delta）の配列を確保\n",
    "        d1 = np.zeros(NUM_HIDDEN)\n",
    "        d2 = np.zeros(NUM_OUTPUT)\n",
    "\n",
    "        # 訓練データにバイアスの1を先頭に挿入\n",
    "        x = np.array([xlist[n]])\n",
    "        x = np.insert(x, 0, 1)\n",
    "\n",
    "        # (1) 順伝播により隠れ層の出力を計算\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            a = np.zeros(NUM_HIDDEN)\n",
    "            a[j] = np.dot(w1[j, :], x)\n",
    "            z[j] = np.tanh(a[j])\n",
    "\n",
    "        # (1) 順伝播により出力層の出力を計算\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            y[k] = np.dot(w2[k, :], z)\n",
    "\n",
    "        #  <b>出力層の誤差を評価</b>\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            d2[k] = y[k] - tlist[n, k] #n,kは行と列に対応\n",
    "\n",
    "　　　　　#　<b>出力層の重みを更新</b>\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            for j in range(NUM_HIDDEN):\n",
    "                w2[k, j] -= ETA * d2[k] * z[j]\n",
    "</pre>\n",
    "\n",
    "$$\n",
    "w_{ij}^{(L)} = w_{ij}^{(L)} - \\eta((y_{i}^{(L)} - t_{i}){f'(u_{i}^{(L)})}{y_{j}^{(L-1)}})\n",
    "$$\n",
    "\n",
    "<pre>\n",
    "        #  <b>出力層の誤差を逆伝播させ、隠れ層の誤差を計算</b>\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            temp = np.dot(w2[:, j], d2)\n",
    "            d1[j] = (1 - z[j] * z[j]) * temp\n",
    "\n",
    "        #  <b>隠れ層の重みを更新</b>\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            for i in range(NUM_INPUT):\n",
    "                w1[j, i] -= ETA * d1[j] * x[i]\n",
    "</pre>\n",
    "\n",
    "$$\n",
    "w_{ij}^{(L-1)} = w_{ij}^{(L-1)} - \\eta(\\sum_{k} \\frac{\\partial E_n}{\\partial y_{k}^{(L)}}\\frac{\\partial y_{k}^{(L)}}{\\partial u_{k}^{(L)}}w_{k}^{(L)}{f'(u_{i}^{(L-1)})}{y_{j}^{(L-2)}})\n",
    "$$\n",
    "\n",
    "<pre>\n",
    "    # 訓練データの入力に対する予測値を出力\n",
    "    ylist = np.zeros((N, NUM_OUTPUT))\n",
    "    zlist = np.zeros((N, NUM_HIDDEN))\n",
    "    for n in range(N):\n",
    "        ylist[n], zlist[n] = output(xlist[n], w1, w2)\n",
    "\n",
    "    # 訓練データを青丸で表示\n",
    "    # plot()には縦ベクトルを渡してもOK\n",
    "    im = plt.plot(xlist, tlist, 'bo')\n",
    "\n",
    "    # 訓練データの各xに対するニューラルネットの出力を赤線で表示\n",
    "    im += plt.plot(xlist, ylist, 'r-')\n",
    "\n",
    "    # 各重みの出力を点線で表示\n",
    "    for i in range(NUM_HIDDEN):\n",
    "        im += plt.plot(xlist, zlist[:,i], 'k--')\n",
    "    \n",
    "    ims.append(im)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 動画を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "fig = plt.figure()\n",
    "ani = animation.ArtistAnimation(fig, ims)\n",
    "plt.show()\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 訓練データ数\n",
    "N = 50\n",
    "# 学習率\n",
    "ETA = 0.1\n",
    "# ループ回数\n",
    "NUM_LOOP = 500\n",
    "# 入力層のユニット数（バイアス含む）\n",
    "NUM_INPUT = 2\n",
    "# 隠れ層のユニット数（バイアス含む）\n",
    "NUM_HIDDEN = 4\n",
    "# 出力層のユニット数\n",
    "NUM_OUTPUT = 1\n",
    "\n",
    "def output(x, w1, w2):\n",
    "    \"\"\"xを入力したときのニューラルネットワークの出力を計算\n",
    "    隠れユニットの出力も一緒に返す\"\"\"\n",
    "    # 配列に変換して先頭にバイアスの1を挿入\n",
    "    x = np.insert(x, 0, 1)\n",
    "    z = np.zeros(NUM_HIDDEN)\n",
    "    y = np.zeros(NUM_OUTPUT)\n",
    "    # 順伝播で出力を計算\n",
    "    for j in range(NUM_HIDDEN):\n",
    "        a = np.zeros(NUM_HIDDEN)\n",
    "        a[j] = np.dot(w1[j, :], x)\n",
    "        z[j] = np.tanh(a[j])\n",
    "    for k in range(NUM_OUTPUT):\n",
    "        y[k] = np.dot(w2[k, :], z)\n",
    "    return y, z\n",
    "\n",
    "# 訓練データ作成\n",
    "xlist = np.linspace(-5, 5, N).reshape(N, 1)\n",
    "#tlist = xlist * xlist    # x^2\n",
    "tlist = np.sin(xlist)     # sin(x)\n",
    "#tlist = np.abs(xlist)    # |x|\n",
    "#tlist = heviside(xlist)  # H(x)\n",
    "\n",
    "# 重みをランダムに初期化\n",
    "w1 = np.random.random((NUM_HIDDEN, NUM_INPUT))\n",
    "w2 = np.random.random((NUM_OUTPUT, NUM_HIDDEN))\n",
    "\n",
    "#print \"学習前の二乗誤差:\", sum_of_squares_error(xlist, tlist, w1, w2)\n",
    "\n",
    "ims = []\n",
    "\n",
    "for loop in range(NUM_LOOP):\n",
    "    # 訓練データすべてを使って重みを訓練する\n",
    "    for n in range(N):\n",
    "        # 隠れ層と出力層の出力配列を確保\n",
    "        z = np.zeros(NUM_HIDDEN)\n",
    "        y = np.zeros(NUM_OUTPUT)\n",
    "\n",
    "        # 誤差（delta）の配列を確保\n",
    "        d1 = np.zeros(NUM_HIDDEN)\n",
    "        d2 = np.zeros(NUM_OUTPUT)\n",
    "\n",
    "        # 訓練データにバイアスの1を先頭に挿入\n",
    "        x = np.array([xlist[n]])\n",
    "        x = np.insert(x, 0, 1)\n",
    "\n",
    "        # (1) 順伝播により隠れ層の出力を計算\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            # 入力層にはバイアスの1が先頭に入るので注意\n",
    "            a = np.zeros(NUM_HIDDEN)\n",
    "            a[j] = np.dot(w1[j, :], x)\n",
    "            z[j] = np.tanh(a[j])\n",
    "\n",
    "        # (1) 順伝播により出力層の出力を計算\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            y[k] = np.dot(w2[k, :], z)\n",
    "\n",
    "        # (2) 出力層の誤差を評価\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            d2[k] = y[k] - tlist[n, k]\n",
    "\n",
    "        # (3) 出力層の誤差を逆伝播させ、隠れ層の誤差を計算\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            temp = np.dot(w2[:, j], d2)\n",
    "            d1[j] = (1 - z[j] * z[j]) * temp\n",
    "\n",
    "        # (4) + (5) 第1層の重みを更新\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            for i in range(NUM_INPUT):\n",
    "                w1[j, i] -= ETA * d1[j] * x[i]\n",
    "            \n",
    "        # (4) + (5) 第2層の重みを更新\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            for j in range(NUM_HIDDEN):\n",
    "                w2[k, j] -= ETA * d2[k] * z[j]\n",
    "\n",
    "    # 訓練データの入力に対する予測値を出力\n",
    "    ylist = np.zeros((N, NUM_OUTPUT))\n",
    "    zlist = np.zeros((N, NUM_HIDDEN))\n",
    "    for n in range(N):\n",
    "        ylist[n], zlist[n] = output(xlist[n], w1, w2)\n",
    "\n",
    "    # 訓練データを青丸で表示\n",
    "    # plot()には縦ベクトルを渡してもOK\n",
    "    im = plt.plot(xlist, tlist, 'bo')\n",
    "\n",
    "    # 訓練データの各xに対するニューラルネットの出力を赤線で表示\n",
    "    im += plt.plot(xlist, ylist, 'r-')\n",
    "\n",
    "    # 各重みの出力を点線で表示\n",
    "    for i in range(NUM_HIDDEN):\n",
    "        im += plt.plot(xlist, zlist[:,i], 'k--')\n",
    "    \n",
    "    ims.append(im)\n",
    "\n",
    "fig = plt.figure()\n",
    "ani = animation.ArtistAnimation(fig, ims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chainerによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なものをimport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<pre>\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import numpy as np\n",
    "from chainer import Variable\n",
    "from chainer import optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<pre>\n",
    "N = 50\n",
    "NUM_LOOP = 1000\n",
    "x_data = np.linspace(-1, 1, N, dtype=np.float32).reshape(N,1)\n",
    "t_data = x_data * x_data    # x^2\n",
    "#    tlist = np.sin(xlist)    # sin(x)\n",
    "#    tlist = np.abs(xlist)    # |x|\n",
    "#    tlist = heviside(xlist)  # H(x)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "<pre>\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_in, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "            l1=L.Linear(n_in,n_units),\n",
    "            l2=L.Linear(n_units,n_out)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h1 = F.tanh(self.l1(x))\n",
    "        h2 = self.l2(h1)\n",
    "        #h1 = F.leaky_relu(self.l1(x))\n",
    "        #h2 = F.leaky_relu(self.l2(h1))\n",
    "        return h2\n",
    "\n",
    "class SUMError(chainer.Chain):\n",
    "    def __init__(self, predictor):\n",
    "        l1=L.Linear(n_in,n_units),\n",
    "            l2=L.Linear(n_units,n_out)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        self.loss = F.mean_squared_error(y, t) //Mean squared error (a.k.a. Euclidean loss) function\n",
    "        return self.loss\n",
    "        \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizerを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<pre>\n",
    "model = SUMError(MLP(1, 4, 1))\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "</pre>\n",
    "\n",
    "他のパラメータとその意味は、[ここ](http://ttlg.hateblo.jp/entry/2016/02/11/181322) で参考できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainを行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<pre>\n",
    "losses = []\n",
    "for loop in range(NUM_LOOP):\n",
    "    x = Variable(x_data)\n",
    "    t = Variable(t_data.astype(np.float32))\n",
    "    model.zerograds()\n",
    "    loss = model(x, t)\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    losses.append(loss.data)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAECCAYAAAAciLtvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105bbdd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import numpy as np\n",
    "from chainer import Variable\n",
    "from chainer import optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "N = 50\n",
    "NUM_LOOP = 5000\n",
    "x_data = np.linspace(-1, 1, N, dtype=np.float32).reshape(N,1)\n",
    "t_data = x_data * x_data        # x^2\n",
    "#    t_data = np.sin(x_data)    # sin(x)\n",
    "#    t_data = np.abs(x_data)    # |x|\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_in, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "            l1=L.Linear(n_in,n_units),\n",
    "            l2=L.Linear(n_units,n_out)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h1 = F.tanh(self.l1(x))\n",
    "        h2 = self.l2(h1)\n",
    "        #h1 = F.leaky_relu(self.l1(x))\n",
    "        #h2 = F.leaky_relu(self.l2(h1))\n",
    "        return h2\n",
    "\n",
    "class SUMError(chainer.Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(SUMError, self).__init__(predictor=predictor)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        self.loss = F.mean_squared_error(y, t)\n",
    "        return self.loss\n",
    "    \n",
    "model = SUMError(MLP(1, 4, 1))\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "losses = []\n",
    "for loop in range(NUM_LOOP):\n",
    "    x = Variable(x_data)\n",
    "    t = Variable(t_data.astype(np.float32))\n",
    "    model.zerograds()\n",
    "    loss = model(x, t)\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    losses.append(loss.data)\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
